import streamlit as st
from dotenv import load_dotenv
import os
import logging
import google.generativeai as genai
from langchain.callbacks.base import BaseCallbackHandler
from langchain.chat_models import ChatOpenAI
from langchain.schema import ChatMessage
from langchain_google_genai import ChatGoogleGenerativeAI

# ãƒ­ã‚¬ãƒ¼ã®è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# APIã‚­ãƒ¼ã®è¨­å®š
load_dotenv(os.path.join(os.path.dirname(__file__), '..', '.env'))
google_gemini_key = os.environ["GOOGLE_GEMINI_KEY"]
genai.configure(api_key=google_gemini_key)

# Streamlitã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã®åˆæœŸåŒ–
if "chat_log" not in st.session_state:
    st.session_state.chat_log = []

# if "messages" not in st.session_state:
#     st.session_state["messages"] = [ChatMessage(role="assistant", content="ãªã‚“ã§ã‚‚èã„ã¦ã­")]

class StreamHandler(BaseCallbackHandler):
    def __init__(self, container, initial_text=""):
        self.container = container
        self.text = initial_text

    def on_llm_new_token(self, token: str, **kwargs) -> None:
        self.text += token
        self.container.markdown(self.text)

# Streamlitã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹è¨­å®š
# st.title('ğŸ˜±Generative AI with Google API')

# for msg in st.session_state.messages:
#     st.chat_message(msg.role).write(msg.content)

# if prompt := st.chat_input():
#     st.session_state.messages.append(ChatMessage(role="user", content=prompt))
#     st.chat_message("user").write(prompt)

#     with st.chat_message("assistant"):
#         try:
#             #stream_handler = StreamHandler(st.empty())
#             llm = ChatGoogleGenerativeAI(model="gemini-pro")
#             print(st.session_state.messages)
#             # contentã¨roleã‚’å–ã‚Šå‡ºã—ã¦çµåˆ
#             formatted_messages = ["[{}]: {}".format(message.role, message.content) for message in st.session_state.messages]

#             # ä¸€ã¤ã®æ–‡å­—åˆ—ã«çµåˆ
#             joined_string = "\n".join(formatted_messages)

#             print(joined_string)
#             response = llm.invoke(joined_string)
#             st.session_state.messages.append(ChatMessage(role="assistant", content=response.content))
#         except Exception as e:
#             logger.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}", exc_info=True)
#             st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

# Streamlitã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹è¨­å®š
st.title('ğŸ˜±Generative AI with Google API')

# æ—¢å­˜ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
for msg in st.session_state.messages:
    st.chat_message(msg.role).write(msg.content)

# ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã®å‡¦ç†
if prompt := st.chat_input():
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ 
    st.session_state.messages.append(ChatMessage(role="user", content=prompt))

    # AIãƒ¢ãƒ‡ãƒ«ã§å¿œç­”ã‚’ç”Ÿæˆ
    try:
        llm = ChatGoogleGenerativeAI(model="gemini-pro")
        formatted_messages = ["[{}]: {}".format(message.role, message.content) for message in st.session_state.messages]
        joined_string = "\n".join(formatted_messages)
        response = llm.invoke(joined_string)
        
        # AIã®å¿œç­”ã‚’ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒªã‚¹ãƒˆã«è¿½åŠ 
        st.session_state.messages.append(ChatMessage(role="assistant", content=response.content))
    except Exception as e:
        logger.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}", exc_info=True)
        st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

# ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®è¡¨ç¤ºã‚’æ›´æ–°
for msg in st.session_state.messages:
    st.chat_message(msg.role).write(msg.content)

