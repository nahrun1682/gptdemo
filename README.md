# 1. ãƒ‡ãƒ¢ç”¨ãƒ•ãƒ­ãƒ³ãƒˆç”»é¢

# 2. ç›®æ¬¡

- [1. ãƒ‡ãƒ¢ç”¨ãƒ•ãƒ­ãƒ³ãƒˆç”»é¢](#1-ãƒ‡ãƒ¢ç”¨ãƒ•ãƒ­ãƒ³ãƒˆç”»é¢)
- [2. ç›®æ¬¡](#2-ç›®æ¬¡)
- [3. å‰æ](#3-å‰æ)
- [4. ä½¿ã„æ–¹](#4-ä½¿ã„æ–¹)
- [5. ãƒ—ãƒ­ã‚­ã‚·ç’°å¢ƒ](#5-ãƒ—ãƒ­ã‚­ã‚·ç’°å¢ƒ)
- [6. Azure App Servicesã§ã®Streamlitã‚¢ãƒ—ãƒªã®ãƒ‡ãƒ—ãƒ­ã‚¤](#6-azure-app-servicesã§ã®streamlitã‚¢ãƒ—ãƒªã®ãƒ‡ãƒ—ãƒ­ã‚¤)
- [7. Azure App Service ã§å®Ÿè¡Œã•ã‚Œã¦ã„ã‚‹ Web ã‚¢ãƒ—ãƒªã«ã‚¢ãƒ—ãƒªã®èªè¨¼ã‚’è¿½åŠ ã™ã‚‹](#7-azure-app-service-ã§å®Ÿè¡Œã•ã‚Œã¦ã„ã‚‹-web-ã‚¢ãƒ—ãƒªã«ã‚¢ãƒ—ãƒªã®èªè¨¼ã‚’è¿½åŠ ã™ã‚‹)
- [8. Poetryã‚³ãƒãƒ³ãƒ‰](#8-poetryã‚³ãƒãƒ³ãƒ‰)
- [9. Docker](#9-docker)
- [10. k8s](#10-k8s)
- [11. fastapi+gunicorn](#11-fastapigunicorn)
- [12. å®¿é¡Œ](#12-å®¿é¡Œ)
- [13. ã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ—ã‚³ãƒãƒ³ãƒ‰](#13-ã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ—ã‚³ãƒãƒ³ãƒ‰)

# 3. å‰æ 

# 4. ä½¿ã„æ–¹

```zsh
#gptdemoé…ä¸‹ã«ç§»å‹•
cd gptdemo 

#ä»®æƒ³ç’°å¢ƒèµ·å‹•
poetry install
#ãƒ•ãƒ­ãƒ³ãƒˆèµ·å‹•
poetry run streamlit run gptdemo/01_ChatGPT_DEMO.py
```

æˆåŠŸã™ã‚‹ã¨ä»¥ä¸‹ãŒæ¨™æº–å‡ºåŠ›ã•ã‚Œã‚‹
```zsh
/mnt/d/GPT/gptdemo % poetry run streamlit run gptdemo/01_ChatGPT_DEMO.py

  You can now view your Streamlit app in your browser.

  Local URL: http://localhost:8501
  Network URL: http://172.18.62.148:8501

gio: http://localhost:8501: Operation not supported

```

ãã—ãŸã‚‰ã€http://localhost:8501
ã¸é·ç§»

# 5. ãƒ—ãƒ­ã‚­ã‚·ç’°å¢ƒ

å‚è€ƒï¼š[ãƒ—ãƒ­ã‚­ã‚·ç’°å¢ƒã§ã®Pythonç’°å¢ƒæ§‹ç¯‰ã¾ã¨ã‚](https://qiita.com/c60evaporator/items/7a757134d028a7734118)

ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå†…ã®pyproject.tomlã«ä»¥ä¸‹ã®è¨˜è¿°ã‚’è¿½åŠ 

```zsh
[[tool.poetry.source]]
name = "proxy"
url = "http://ãƒ—ãƒ­ã‚­ã‚·ã®ã‚¢ãƒ‰ãƒ¬ã‚¹:ãƒãƒ¼ãƒˆ"
default = true
```

# 6. Azure App Servicesã§ã®Streamlitã‚¢ãƒ—ãƒªã®ãƒ‡ãƒ—ãƒ­ã‚¤
å‚è€ƒ[Azure App Servicesã§ã®Streamlitã‚¢ãƒ—ãƒªã®ãƒ‡ãƒ—ãƒ­ã‚¤](https://docs.kanaries.net/ja/tutorials/Streamlit/deploy-streamlit-app)

[Azure App Serviceã¸ã®Python Webã‚¢ãƒ—ãƒªã®ç°¡å˜ãƒ‡ãƒ—ãƒ­ã‚¤ï¼ˆCLIï¼‰](https://qiita.com/yakigac/items/a3369bfc2f4730cd299f)

Azure App Servicesã«Streamlitã‚¢ãƒ—ãƒªã‚’ãƒ‡ãƒ—ãƒ­ã‚¤ã™ã‚‹æ‰‹é †ã®ã‚¬ã‚¤ãƒ‰ã‚’ä»¥ä¸‹ã«ç¤ºã—ã¾ã™ã€‚

1. Azureã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®ä½œæˆï¼šã¾ã æŒã£ã¦ã„ãªã„å ´åˆã€Azureã‚¢ã‚«ã‚¦ãƒ³ãƒˆã«ã‚µã‚¤ãƒ³ã‚¢ãƒƒãƒ—ã—ã¦ãã ã•ã„ã€‚

1. æ–°ã—ã„App Serviceã®ä½œæˆï¼šAzureãƒãƒ¼ã‚¿ãƒ«ã«ç§»å‹•ã—ã€æ–°ã—ã„App Serviceã‚’ä½œæˆã—ã¾ã™ã€‚

1. App Serviceã®æ§‹æˆï¼šã‚µãƒ–ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³ã€ãƒªã‚½ãƒ¼ã‚¹ã‚°ãƒ«ãƒ¼ãƒ—ã€åå‰ã€å…¬é–‹æ–¹æ³•ï¼ˆã‚³ãƒ¼ãƒ‰ï¼‰ã€ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒƒã‚¯ï¼ˆPythonï¼‰ã€ãŠã‚ˆã³ã‚ªãƒšãƒ¬ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ã‚’é¸æŠã—ã¾ã™ã€‚

1. ã‚¢ãƒ—ãƒªã‚’ãƒ‡ãƒ—ãƒ­ã‚¤ã™ã‚‹ï¼šApp ServiceãŒè¨­å®šã•ã‚ŒãŸã‚‰ã€Azure CLIã¾ãŸã¯Gitã‚’ä½¿ç”¨ã—ã¦Streamlitã‚¢ãƒ—ãƒªã‚’ãƒ‡ãƒ—ãƒ­ã‚¤ã§ãã¾ã™ã€‚Azure CLIã‚’ä½¿ç”¨ã—ã¦ã‚¢ãƒ—ãƒªã‚’ãƒ‡ãƒ—ãƒ­ã‚¤ã™ã‚‹ãŸã‚ã®ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒãƒ³ãƒ‰ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’ä»¥ä¸‹ã«ç¤ºã—ã¾ã™ã€‚

```zsh
az webapp up --sku F1 --name my-streamlit-app
```

ã“ã‚Œã‚‰ã®æ‰‹é †ã®å¾Œã€Streamlitã‚¢ãƒ—ãƒªã¯Azure App Servicesä¸Šã§å…¬é–‹ã•ã‚Œã‚‹ã¯ãšã§ã™ã€‚

# 7. Azure App Service ã§å®Ÿè¡Œã•ã‚Œã¦ã„ã‚‹ Web ã‚¢ãƒ—ãƒªã«ã‚¢ãƒ—ãƒªã®èªè¨¼ã‚’è¿½åŠ ã™ã‚‹

å‚è€ƒï¼š[ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«: Azure App Service ã§å®Ÿè¡Œã•ã‚Œã¦ã„ã‚‹ Web ã‚¢ãƒ—ãƒªã«ã‚¢ãƒ—ãƒªã®èªè¨¼ã‚’è¿½åŠ ã™ã‚‹](https://learn.microsoft.com/ja-jp/azure/app-service/scenario-secure-app-authentication-app-service)



# 8. Poetryã‚³ãƒãƒ³ãƒ‰

å‚è€ƒ:[Poetryã‚’ã‚µã‚¯ãƒƒã¨ä½¿ã„å§‹ã‚ã¦ã¿ã‚‹](https://qiita.com/ksato9700/items/b893cf1db83605898d8a)
ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰ï¼ˆãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚¢ãƒƒãƒ—ï¼‰ã‚’è¡Œã„ãŸã„æ™‚ã«ã¯ poetry updateã‚’ä½¿ã†ã€‚

```zsh
poetry update --dry-run
#ã¨ã™ã‚‹ã¨ã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰ã•ã‚Œã‚‹ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãŒã‚ã‹ã‚‹ã®ã§ã€ãã‚Œã‚’ç¢ºèªã—ãŸä¸Šã§
poetry update
```
ã™ã‚‹ã¨å®Ÿéš›ã«ã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰ãŒè¡Œã‚ã‚Œã¾ã™ã€‚ãªãŠã€poetry updateã—ãŸæ™‚ã«å¤‰æ›´ã•ã‚Œã‚‹ã®ã¯poetry.lockã ã‘ã§ pyproject.tomlã¯ãã®ã¾ã¾ã§ã™ã€‚æ–°ã—ã„ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®æ–°æ©Ÿèƒ½ã‚’ä½¿ã†ãªã©ã®å ´åˆã¯pyproject.tomlã‚’æ‰‹å‹•ã§ä¿®æ­£ã—ã¦ã—ã¦ä¾å­˜ã™ã‚‹ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’å¤‰ãˆã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

# 9. Docker
ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã§Dockerã‚³ãƒ³ãƒ†ãƒŠã‚’ãƒ“ãƒ«ãƒ‰ã—ã¦å®Ÿè¡Œã™ã‚‹ã«ã¯ã€ä»¥ä¸‹ã®æ‰‹é †ã‚’å®Ÿè¡Œã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãŒã‚³ãƒ³ãƒ†ãƒŠå†…ã§ã©ã®ã‚ˆã†ã«å‹•ä½œã™ã‚‹ã‹ã‚’ç¢ºèªã§ãã¾ã™ã€‚ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã¯ã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆ`Dockerfile`ãŒå­˜åœ¨ã™ã‚‹å ´æ‰€ï¼‰ã§å®Ÿè¡Œã™ã‚‹ã“ã¨ã‚’æƒ³å®šã—ã¦ã„ã¾ã™ã€‚

1. **Dockerã‚¤ãƒ¡ãƒ¼ã‚¸ã®ãƒ“ãƒ«ãƒ‰:**
    æœ€åˆã«ã€Dockerã‚¤ãƒ¡ãƒ¼ã‚¸ã‚’ãƒ“ãƒ«ãƒ‰ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆãŒä½œæˆã•ã‚Œã€ãã‚Œã‚’å®Ÿè¡Œã™ã‚‹ãŸã‚ã®ç’°å¢ƒãŒè¨­å®šã•ã‚Œã¾ã™ã€‚ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚

    ```sh
    docker build -t gptdemo .
    ```

    ã“ã“ã§ã€`my-python-app`ã¯ä½œæˆã™ã‚‹Dockerã‚¤ãƒ¡ãƒ¼ã‚¸ã«ä»˜ã‘ã‚‹åå‰ã§ã™ï¼ˆä»»æ„ã®åå‰ã‚’ä½¿ç”¨ã§ãã¾ã™ï¼‰ã€‚ã¾ãŸã€æœ€å¾Œã®`.`ã¯`Dockerfile`ã¨ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼ˆã‚³ãƒ”ãƒ¼ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ãªã©ï¼‰ãŒç¾åœ¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã‚ã‚‹ã“ã¨ã‚’æŒ‡ã—ã¾ã™ã€‚

2. **ã‚³ãƒ³ãƒ†ãƒŠã®å®Ÿè¡Œ:**
    ã‚¤ãƒ¡ãƒ¼ã‚¸ãŒãƒ“ãƒ«ãƒ‰ã•ã‚ŒãŸã‚‰ã€æ–°ã—ã„ã‚³ãƒ³ãƒ†ãƒŠã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’èµ·å‹•ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãŒã‚¦ã‚§ãƒ–ã‚µãƒ¼ãƒãƒ¼ã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹å ´åˆã€é©åˆ‡ãªãƒãƒ¼ãƒˆã‚’å…¬é–‹ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

    ```bash
    docker run --name mygptdemo -p 8501:8080 gptdemo
    ```

    ã“ã®ã‚³ãƒãƒ³ãƒ‰ã¯ã€ãƒ­ãƒ¼ã‚«ãƒ«ãƒã‚·ãƒ³ã®ãƒãƒ¼ãƒˆ8501ã‚’ã‚³ãƒ³ãƒ†ãƒŠã®ãƒãƒ¼ãƒˆ8501ã«ãƒã‚¤ãƒ³ãƒ‰ã—ã¾ã™ï¼ˆStreamlitãŒãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ä½¿ç”¨ï¼‰ã€‚`my-python-app`ã¯ã€å…ˆã»ã©ãƒ“ãƒ«ãƒ‰ã—ãŸDockerã‚¤ãƒ¡ãƒ¼ã‚¸ã®åå‰ã§ã™ã€‚

    ã‚‚ã—ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãŒç•°ãªã‚‹ãƒãƒ¼ãƒˆã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã€`-p`ãƒ•ãƒ©ã‚°ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãã®ãƒãƒ¼ãƒˆç•ªå·ã«å¤‰æ›´ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ï¼ˆä¾‹ï¼š`-p 8080:8080`ï¼‰ã€‚

3. **ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ç¢ºèª:**
    ã‚³ãƒ³ãƒ†ãƒŠãŒå®Ÿè¡Œã•ã‚ŒãŸã‚‰ã€ã‚¦ã‚§ãƒ–ãƒ–ãƒ©ã‚¦ã‚¶ã‚’é–‹ã„ã¦ `http://localhost:8501` ï¼ˆã¾ãŸã¯é¸æŠã—ãŸãƒãƒ¼ãƒˆç•ªå·ã«å¿œã˜ã¦é©åˆ‡ãªURLï¼‰ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã¾ã™ã€‚ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãŒæ­£ã—ãå®Ÿè¡Œã•ã‚Œã¦ã„ã‚Œã°ã€ãã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚

ã“ã®ãƒ—ãƒ­ã‚»ã‚¹ã«ã‚ˆã‚Šã€Dockerã‚³ãƒ³ãƒ†ãƒŠã®ä¸­ã§ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãŒã©ã®ã‚ˆã†ã«å‹•ä½œã™ã‚‹ã‹ã‚’ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã§ç¢ºèªã§ãã¾ã™ã€‚ã“ã‚Œã¯ãƒ‡ãƒ—ãƒ­ã‚¤å‰ã®ãƒ†ã‚¹ãƒˆã‚„ãƒ‡ãƒãƒƒã‚°ã€é–‹ç™ºãƒ—ãƒ­ã‚»ã‚¹ä¸­ã®å•é¡Œã®ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã«éå¸¸ã«å½¹ç«‹ã¡ã¾ã™ã€‚

4. **ã‚¢ã‚¯ã‚»ã‚¹**
   http://localhost:8501

# 10. k8s



# 11. fastapi+gunicorn

```bash
#èµ·å‹•æ–¹æ³•
poetry run gunicorn -w 4 -k uvicorn.workers.UvicornWorker fastapi_app.main:app
```


# 12. å®¿é¡Œ

1. ã‚¹ãƒˆãƒªãƒ¼ãƒ å‡¦ç†ã‚’langcahinã‹ã‚‰
2. langcahinã‹ã‚‰azureã¸
3. dockerã«ã‚ˆã‚‹ã‚³ãƒ³ãƒ†ãƒŠåŒ–
4. jupyter vscodeä¸Šã§ä½¿ã„ãŸã„ã‚ˆã­
5. pip install poetry && poetry install && poetry run streamlit run gptdemo/01_ChatGPT_DEMO.py --server.port $PORT
6. /mnt/d/GPT/GPTDEMO # poetry --version
Poetry (version 1.7.0)

# 13. ã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ—ã‚³ãƒãƒ³ãƒ‰
pip install --upgrade pip && \
pip install poetry==1.7.0 && \  # ã“ã“ã§ç‰¹å®šã®poetryãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’æŒ‡å®š
poetry config virtualenvs.create false --local && \  # ã‚·ã‚¹ãƒ†ãƒ ã®Pythonç’°å¢ƒã‚’ä½¿ç”¨
poetry install --no-interaction --no-ansi && \
poetry run streamlit run gptdemo/01_ChatGPT_DEMO.py --server.port $PORT


pip install --upgrade pip && pip install poetry==1.7.0 && poetry config virtualenvs.create false --local && poetry install --no-interaction --no-ansi && poetry update langchain && poetry run streamlit run gptdemo/01_ChatGPT_DEMO.py --server.port $PORT

```bash
#20231112(å‹•ã„ãŸ)
pip install poetry && poetry install && poetry run streamlit run gptdemo/01_ChatGPT_DEMO.py --server.port $PORT

#20231112()
pip install poetry && poetry install & poetry run gunicorn -w 4 -k uvicorn.workers.UvicornWorker fastapi_app.main:app --bind 0.0.0.0:8000

#ä»¥ä¸‹ã‚¨ãƒ©ãƒ¼
2023-11-12T15:01:47.956Z INFO  - docker run -d --expose=8181 --expose=8082 --name gpt-demo-stremlit_0_eeaa3578_middleware -e WEBSITE_AUTH_ENABLED=True -e PORT=80 -e WEBSITE_SITE_NAME=gpt-demo-stremlit -e WEBSITE_ROLE_INSTANCE_ID=0 -e WEBSITE_HOSTNAME=gpt-demo-stremlit.azurewebsites.net -e WEBSITE_INSTANCE_ID=854dd9506899c9694307e0d08f4d78a285a46de0e3c0d6c64653dc76fa8620f8 -e HTTP_LOGGING_ENABLED=1 -e WEBSITE_USE_DIAGNOSTIC_SERVER=False mcr.microsoft.com/appsvc/middleware:stage5 /Host.ListenUrl=http://0.0.0.0:8181 /Host.DestinationHostUrl=http://169.254.129.2:80 /Host.UseFileLogging=true
```

```bash
sh startup.sh
```

```bash
echo "Starting setup..." ; \
echo "Installing dependencies..." ; pip install poetry ; poetry install ; \
echo "Starting Streamlit application..." ; poetry run streamlit run gptdemo/01_ChatGPT_DEMO.py --server.port 80 & \
echo "Starting FastAPI application..." ; poetry run gunicorn -w 4 -k uvicorn.workers.UvicornWorker fastapi_app.main:app --bind 0.0.0.0:8000 & ; \
echo "Startup script completed."
```

#é›‘è¨˜

```python
ãƒ¡ãƒ¢ã€€éåŒæœŸå‡¦ç†

import multiprocessing
import time
import os
from dotenv import load_dotenv
import streamlit as st

from langchain.callbacks.base import BaseCallbackHandler
from langchain.chat_models import ChatOpenAI
from langchain.schema import ChatMessage

# .envãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
load_dotenv(os.path.join(os.path.dirname(__file__), '..', '.env'))
openai_api_key = os.environ["OPENAI_API_KEY"]

class LLMWorker(multiprocessing.Process):
    def __init__(self, openai_api_key, model_name, temperature, messages, **kwargs):
        super().__init__(**kwargs)
        self.openai_api_key = openai_api_key
        self.model_name = model_name
        self.temperature = temperature
        self.messages = messages
        self.response = multiprocessing.Queue(1)

    def run(self):
        llm = ChatOpenAI(openai_api_key=self.openai_api_key, model_name=self.model_name, temperature=self.temperature)
        response = llm(self.messages)
        self.response.put(response)

def main():
    st.title('ğŸ¦œChatGPT DEMO')

    with st.sidebar:
        st.header('è¨­å®š')
        with st.expander("ãƒ¢ãƒ‡ãƒ«é¸æŠ"):
            model_name = st.radio(
                "ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ(1106ãŒç¾åœ¨æœ€æ–°ç‰ˆ):",
                ("gpt-3.5-turbo", "gpt-4", "gpt-3.5-turbo-1106", "gpt-4-1106-preview"),
                index=3
            )
        with st.expander("ã‚ªãƒ—ã‚·ãƒ§ãƒ³è¨­å®š"):
            temperature = st.slider(
                "Temperature(å¤§ãã„ã»ã©æ­£ç¢ºã€ä½ã„ã»ã©ãƒ©ãƒ³ãƒ€ãƒ ):", 
                min_value=0.0, max_value=1.0, value=1.0, step=0.1
            )

    if "messages" not in st.session_state:
        st.session_state["messages"] = [ChatMessage(role="assistant", content="ãªã‚“ã§ã‚‚èã„ã¦ã­")]

    for msg in st.session_state.messages:
        st.chat_message(msg.role).write(msg.content)

    if prompt := st.chat_input():
        st.session_state.messages.append(ChatMessage(role="user", content=prompt))
        st.chat_message("user").write(prompt)

        worker = LLMWorker(openai_api_key, model_name, temperature, st.session_state.messages, daemon=True)
        worker.start()

        while worker.is_alive():
            time.sleep(0.1)

        response = worker.response.get()
        st.session_state.messages.append(ChatMessage(role="assistant", content=response.content))
        worker.join()

if __name__ == '__main__':
    main()

```

ä»¥ä¸‹ã¯streamã—ã¤ã¤éåŒæœŸå‡¦ç†

```python
from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder
from langchain.chat_models import ChatOpenAI
from langchain.memory import ConversationBufferMemory
import streamlit as st
import asyncio

st.set_page_config(layout="wide")
async def manual_run(panel, llm, prompt_template, memory, question):
    message = prompt_template.format_prompt(
        human_input=question, 
        chat_history=memory.load_memory_variables({})["chat_history"]
    )
    # add to ui
    with panel:
        with st.chat_message("user"):
            st.markdown(question)
        with st.chat_message("assistant"):
            container = st.empty()

    response = ""
    async for chunk in llm.astream(message):
        response += chunk.content
        container.markdown(response)

    memory.save_context({"input": question}, {"output": response})
    return response


async def logic(location, key, panel):
    llm = ChatOpenAI(model="gpt-3.5-turbo-0613", temperature=0.2, max_tokens=512, streaming=True)
    prompt = ChatPromptTemplate.from_messages([
        MessagesPlaceholder(variable_name="chat_history"),
        HumanMessagePromptTemplate.from_template("{human_input}")
    ])
    memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

    questions = [
        "ã“ã‚“ã«ã¡ã¯",
        f"{location}ã®è¦³å…‰åæ‰€ã‚’æ•™ãˆã¦",
        f"ã“ã®è¦³å…‰åæ‰€ã®{location}é§…ã‹ã‚‰ã®è¡Œãæ–¹ã¯ï¼Ÿ"
    ]
    answers = []
    for question in questions:
        # Add to state
        st.session_state[f"messages{key}"].append({
            "role": "user",
            "content": question
        })
        # run chatgpt
        ans = await manual_run(panel, llm, prompt, memory, question)
        # add to state
        st.session_state[f"messages{key}"].append({
            "role": "assistant",
            "content": ans
        })
        answers.append(ans)
    return answers

async def task_factory(parameters):
    tasks = []
    for param in parameters:
        t = asyncio.create_task(logic(*param))
        tasks.append(t)
    return await asyncio.gather(*tasks)

def main():
    if "messages1" not in st.session_state:
        st.session_state.messages1 = []
        st.session_state.messages2 = []

    st.title("Multiple Streaming Chat")

    column_left, column_right = st.columns(2)
    button = st.button("Click to start chats")

    with column_left:
        for message in st.session_state.messages1:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])

    with column_right:
        for message in st.session_state.messages2:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])

    if button:
        parameters = [
            ("æ±äº¬", 1, column_left),
            ("äº¬éƒ½", 2, column_right)
        ]

        asyncio.run(task_factory(parameters))

if __name__ == "__main__":
    main()

```

ä»¥ä¸‹ã¯ã‚·ãƒ³ãƒ—ãƒ«ç‰ˆ
```python
import asyncio
from langchain.chat_models import ChatOpenAI
from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder
from langchain.memory import ConversationBufferMemory
import streamlit as st

st.set_page_config(layout="wide")

async def manual_run(panel, llm, prompt_template, memory, question):
    message = prompt_template.format_prompt(
        human_input=question, 
        chat_history=memory.load_memory_variables({})["chat_history"]
    )
    with panel:
        with st.chat_message("user"):
            st.markdown(question)
        with st.chat_message("assistant"):
            container = st.empty()

    response = ""
    async for chunk in llm.astream(message):
        response += chunk.content
        container.markdown(response)

    memory.save_context({"input": question}, {"output": response})
    return response

async def logic(panel):
    llm = ChatOpenAI(model="gpt-3.5-turbo-0613", temperature=0.2, max_tokens=512, streaming=True)
    prompt = ChatPromptTemplate.from_messages([
        MessagesPlaceholder(variable_name="chat_history"),
        HumanMessagePromptTemplate.from_template("{human_input}")
    ])
    memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

    if "messages" not in st.session_state:
        st.session_state.messages = []

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    if prompt_input := st.chat_input():
        st.session_state.messages.append({
            "role": "user",
            "content": prompt_input
        })
        ans = await manual_run(panel, llm, prompt, memory, prompt_input)
        st.session_state.messages.append({
            "role": "assistant",
            "content": ans
        })

def main():
    st.title("Simple Chat with GPT")
    asyncio.run(logic(st.container()))

if __name__ == "__main__":
    main()
```
